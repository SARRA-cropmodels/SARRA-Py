{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple 2 : recalibrating crop simulation parameters based on optimization - North Cameroon Maize 1999-2022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to show how to use the SARRA-Py package to perform multi-year yield simulations for Maize at the scale of northern Cameroon, and to compare results to a reference departemental yield database to back-feed a bayesian optimization algorithm to recalibrate some of the crop simulation parameters based on a criterion of maximization of correlation coefficient between simulated and observed yield. This is a rather advanced example of what can be done quite easily."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm as tqdm\n",
    "import xarray as xr\n",
    "from sarra_py import *\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import shutil\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, we install the joblib package, which is used for parallel computing\n",
    "# also, the package bayesian-optimization which is used for simulation parameter optimization\n",
    "# (uncomment the following lines to trigger installation)\n",
    "# !pip install joblib\n",
    "# !pip install bayesian-optimization\n",
    "\n",
    "# then we import these packages\n",
    "from joblib import Parallel, delayed\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading maize yield observation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will prepare the reference yield dataset that will be used for comparison against simulation results. \n",
    "As we use official departemental yield data, we first load the shapefile of departemental administrative boundaries with geopandas. Then, we load our statistics with pandas. Finally we associate the two objects based on departements names, by computing a text distance metric (Levenshtein)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a Levenshtein distance helper function\n",
    "\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year min: 1999 / Year max: 2022\n"
     ]
    }
   ],
   "source": [
    "# load shapefile of departemental administrative boundaries\n",
    "departements = gpd.read_file(\"../data/exemple_data/exemple_2/shp_division_north_Cmr/Départements_Nord_Extrême-Nord.shp\")\n",
    "\n",
    "# load official departmental maize yields\n",
    "yields = pd.read_csv(\"../data/exemple_data/exemple_2/Nocamy_v1(North_Cameroon_Maize_Yields).csv\", encoding=\"latin-1\", sep=\";\")\n",
    "yields[\"Yield\"] = yields[\"Yield\"].str.replace(\",\", \".\").astype(float) * 1000 # fix decimal separator and convert units to kg/ha\n",
    "\n",
    "# identify year min and year max from yield dataset\n",
    "year_min_rdt = yields[\"Year\"].min()\n",
    "year_max_rdt = yields[\"Year\"].max()\n",
    "print(\"Year min:\", year_min_rdt, \"/ Year max:\", year_max_rdt)\n",
    "\n",
    "# match departments DIVISION name with yields Division name using string resemblance through Levenshtein distance\n",
    "departements[\"Division\"] = departements[\"DIVISION\"].apply(lambda x: yields[\"Division\"].iloc[np.argmin([levenshtein_distance(x, y) for y in yields[\"Division\"]])])\n",
    "\n",
    "# add annual yields columns to the departements dataframe based on the Division name matching\n",
    "for year in range(year_min_rdt, year_max_rdt + 1):\n",
    "    departements[str(year)] = departements[\"Division\"].apply(lambda x: yields[(yields[\"Division\"] == x) & (yields[\"Year\"] == year)][\"Yield\"].values[0] if len(yields[(yields[\"Division\"] == x) & (yields[\"Year\"] == year)]) > 0 else np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a geopandas dataframe that has departement names, geometries, and in columns the yearly yields from the reference dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departements.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing climate and rainfall data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the already-prepared climate and rainfall data from the Zenodo repository. Climate data is from AgERA5, and rainfall is TAMSAT v3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded using urllib.\n",
      "File unzipped.\n"
     ]
    }
   ],
   "source": [
    "# downloading and unzipping the data from https://doi.org/10.5281/zenodo.14614281\n",
    "\n",
    "# create a folder to store the data\n",
    "os.makedirs('../data/exemple_data/exemple_2/', exist_ok=True)\n",
    "\n",
    "# download preformatted data from Zenodo repository\n",
    "url = 'https://zenodo.org/records/14614282/files/North_Cameroon_1990_2022.zip?download=1'\n",
    "local_filename = '../data/exemple_data/exemple_2/North_Cameroon_1990_2022.zip' # store the downloaded file in the ../data/exemple_data/ folder\n",
    "urllib.request.urlretrieve(url, local_filename)\n",
    "print(\"File downloaded using urllib.\")\n",
    "\n",
    "# unzip data\n",
    "path_to_zip_file = \"../data/exemple_data/exemple_2/North_Cameroon_1990_2022.zip\"\n",
    "directory_to_extract_to = \"../data/exemple_data/exemple_2/\" # unzips the file in the same folder\n",
    "\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "print(\"File unzipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing simulation parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copy the provided example ITK and variety parameters in the right folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/params/variety/variety_exemple_2.yaml'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we copy maize_cameroon_2020. yaml from the example folder to the ../data/params/itk/ folder \n",
    "shutil.copy(\"../data/exemple_data/exemple_2/itk_exemple_2.yaml\", \"../data/params/itk/itk_exemple_2.yaml\")\n",
    "\n",
    "# we copy maize_north_cameroon.yaml from the example folder to the ../data/params/variete/ folder\n",
    "shutil.copy(\"../data/exemple_data/exemple_2/variety_exemple_2.yaml\", \"../data/params/variety/variety_exemple_2.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing and launching simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a run_simulation function, which aim is to return the simulated yield for a given year and parameters. The parameters we want to optimize are phenological phase durations SDJBVP, SDJRPR, and SDJMatu1, and also a biomass partitioning parameter KRdtPotA. Being able to launch a full simulation in a function call will be useful later as we parallelize this action to speed up the whole process. The list of functions passed to launch simulations are identical to what is presented in Exemple 1, though it is way more condensed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a simulation function to be able to parallelize\n",
    "def run_simulation(year, KRdtPotA, SDJBVP, SDJRPR, SDJMatu1):\n",
    "\n",
    "    # displaying the year\n",
    "    print(\"===\",year,\"===\")\n",
    "\n",
    "    # hiding all the outputs of this code section\n",
    "    with open(os.devnull, 'w') as devnull, redirect_stdout(devnull), redirect_stderr(devnull):\n",
    "\n",
    "        # defining start date and simulatin duration ; simulation starts on April 1st and lasts until the end of the year\n",
    "        date_start = datetime.date(year,4,1)\n",
    "        duration = 365-(date_start-datetime.date(date_start.year,1,1)).days\n",
    "\n",
    "        rainfall_data_path = \"../data/exemple_data/exemple_2/TAMSAT_v3.1_north_cameroon_rfe_filled\" # path to rainfall data\n",
    "        climate_data_path = \"../data/exemple_data/exemple_2/AgERA5_north_cameroon/\" # path to climate data\n",
    "        grid_width, grid_height = get_grid_size(rainfall_data_path, date_start, duration) # get grid size        \n",
    "        base_data = xr.Dataset() # initialize empty xarray dataset\n",
    "        base_data = load_TAMSAT_data(base_data, rainfall_data_path, date_start, duration) # load rainfall data\n",
    "        base_data = load_AgERA5_data(base_data, climate_data_path, date_start, duration) # load climate data        \n",
    "        base_data = load_iSDA_soil_data_alternate(base_data, grid_width, grid_height) # load soil parameters\n",
    "        base_data = calc_day_length_raster_fast(base_data, date_start, duration) # compute day length raster\n",
    "\n",
    "        # load variety, cropping system and soil parameters\n",
    "        file_paramVariete = \"maize_north_cameroon.yaml\"\n",
    "        file_paramITK = \"maize_cameroon_2020.yaml\"\n",
    "        file_paramTypeSol = \"USA_iowa_V42.yaml\"\n",
    "        paramVariete, paramITK, paramTypeSol = load_YAML_parameters(file_paramVariete, file_paramITK, file_paramTypeSol)\n",
    "\n",
    "        # updating parameters\n",
    "        paramITK[\"DateSemis\"] = date_start # forcing the sowing date to be the start date\n",
    "        paramVariete[\"KRdtPotA\"] = KRdtPotA # forcing the optimization parameters\n",
    "        paramVariete[\"SDJBVP\"] = SDJBVP\n",
    "        paramVariete[\"SDJRPR\"] = SDJRPR\n",
    "        paramVariete[\"SDJMatu1\"] = SDJMatu1\n",
    "\n",
    "        # creating simulation xarray dataset by copying the base data ; initializing all the necessary variables\n",
    "        data = base_data.copy() \n",
    "        data = initialize_simulation(data, grid_width, grid_height, duration, paramVariete, paramITK, date_start)\n",
    "        data = initialize_default_irrigation(data)\n",
    "        data = calculate_once_daily_thermal_time(data, paramVariete)\n",
    "\n",
    "    data = run_model(paramVariete, paramITK, paramTypeSol, data, duration) # running the model\n",
    "    rdt = data[\"rdt\"][-1,:,:] # extracting the simulated yield (last timepoint)\n",
    "\n",
    "    # freeing memory\n",
    "    del base_data\n",
    "    del data\n",
    "\n",
    "    return rdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a black box objective function function that aims at launching crop simulations in parallel, collect multi-year simulated yields, compute the mean of simulated yields inside each department limits, compare it to the official yields and compute a metric - here a Pearson correlation, as SARRA-Py is notoriously known to overestimate yields. However this code also contains calculation of R2 and RMSE metrics if you want to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the objective function\n",
    "def run_sim_and_calc_score(KRdtPotA, SDJBVP, SDJRPR, SDJMatu1):\n",
    "\n",
    "    year_min, year_max = 1999, 2022 # define the interval of years on which to run the simulations\n",
    "    parallel_jobs = 4 # if you have lots of RAM increase the number of parallel jobs\n",
    "    results = Parallel(n_jobs=parallel_jobs)(delayed(run_simulation)(year, KRdtPotA, SDJBVP, SDJRPR, SDJMatu1) for year in range(year_min, year_max+1)) # parallel run the simulations\n",
    "    rdt = xr.concat(results, dim=\"year\") # assemble the results in a single xarray dataset\n",
    "\n",
    "    # loop over the years and departments to calculate the mean and median of the simulated yield for each of them\n",
    "    results = pd.DataFrame() # initialize results dataframe\n",
    "\n",
    "    # loop over the years and departments\n",
    "    for year in tqdm(range(np.max([year_min_rdt, year_min]), np.min([year_max_rdt, year_max]) + 1)):\n",
    "        for i in range(len(departements)):\n",
    "            index_mean = np.mean(rdt[year_max-year,:,:].rio.clip([departements.iloc[i].geometry])).values # compute departmental mean yield\n",
    "            index_median = (rdt[year_max-year,:,:].rio.clip([departements.iloc[i].geometry])).median().values # compute departmental median yield\n",
    "            results = pd.concat([results, pd.DataFrame({\"year\": year, \"departement\": departements.loc[i, \"Division\"], \"rendement\": departements.loc[i, str(year)], \"rdt_sim_mean\": index_mean, \"rdt_sim_median\": index_median}, index=[0])], ignore_index=True) # append results to the dataframe\n",
    "\n",
    "    # calculate metrics\n",
    "    r2 = r2_score(results.dropna()[\"rendement\"], results.dropna()[\"rdt_sim_mean\"])\n",
    "    rmse = np.sqrt(mean_squared_error(results.dropna()[\"rendement\"], results.dropna()[\"rdt_sim_mean\"]))\n",
    "    pearson = np.corrcoef(results.dropna()[\"rendement\"], results.dropna()[\"rdt_sim_mean\"])[0,1]\n",
    "    print(\"R2:\", r2, \"/ RMSE:\", rmse, \"/ Pearson:\", pearson)\n",
    "\n",
    "\n",
    "    return pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the boundaries in which we look for a global optimum for each variable, and launch the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting boundaries\n",
    "# KrdtPotA: between 0.35 and 1.2\n",
    "# SDJBVP: between 200 and 500 \n",
    "# SDJRPR: between 360 and 630\n",
    "# SDJMatu1: between 370 and 900\n",
    "\n",
    "pbounds = {'KRdtPotA': (0.35, 1.2),\n",
    "           'SDJBVP': (200, 500),\n",
    "           'SDJRPR': (360, 630),\n",
    "           'SDJMatu1': (370, 900)}\n",
    "\n",
    "# instantiating the optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=run_sim_and_calc_score,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "# running the optimization, for threee initial points and ten iterations\n",
    "# (we can increase the number of initial points and iterations to improve the optimization)\n",
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the best identified parameters\n",
    "print(optimizer.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
